{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3a2608-51de-4599-bb9e-8ff857d88fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.7.0\n",
      "google-cloud-aiplatform==1.58.0\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, Condition\n",
    "from kfp.registry import RegistryClient\n",
    "from typing import NamedTuple\n",
    "from google_cloud_pipeline_components.v1.endpoint import ModelDeployOp\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db736093-65de-4e6d-9dac-e6b95cdfd776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"demoespecialidadgcp\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = f\"gs://demo_ts\"\n",
    "SERVICE_ACCOUNT = \"502688298240-compute@developer.gserviceaccount.com\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\"\n",
    "DATASET_ID = \"demo_ts\"\n",
    "TABLE_TRAIN = \"train\"\n",
    "TABLE_TEST = \"test\"\n",
    "\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "BQ_REGION = REGION.split(\"-\")[0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffc2dab-56ca-41b9-b183-10e1d2c213b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fcc170-c946-4e80-bc6b-68b5447ce162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery[pandas]==3.25.0\"],\n",
    "    base_image=\"python:3.10.4\"\n",
    ")\n",
    "def export_datasets(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_train: str,\n",
    "    table_test: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        project_id: The Project ID.\n",
    "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
    "        table_train: The BigQuery train table name.\n",
    "        table_test: The BigQuery test table name.\n",
    "        \n",
    "    Returns:\n",
    "        dataset_train: The Dataset artifact with exported CSV file.\n",
    "        dataset_test: The Dataset artifact with exported CSV file.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_name = f\"{project_id}.{dataset_id}.{table_train}\"\n",
    "    query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM {table_name}\n",
    "    \"\"\".format(\n",
    "        table_name=table_name\n",
    "    )\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)    \n",
    "    df_train = query_job.result().to_dataframe()\n",
    "    \n",
    "    table_name = f\"{project_id}.{dataset_id}.{table_test}\"\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_name}\n",
    "    \"\"\".format(\n",
    "        table_name=table_name\n",
    "    )\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)    \n",
    "    df_test = query_job.result().to_dataframe()\n",
    "    \n",
    "    df_train['source'] = 'train'\n",
    "    df_test['source'] = 'test'\n",
    "    \n",
    "    dataset = pd.concat([df_train, df_test])\n",
    "    \n",
    "    train = dataset.loc[dataset['source'] == 'train']\n",
    "    test = dataset.loc[dataset['source'] == 'test']\n",
    "    train.drop('source', axis = 1, inplace = True)\n",
    "    test.drop('source', axis = 1, inplace = True)\n",
    "    \n",
    "    train.to_csv(dataset_train.path + '.csv', index=False)\n",
    "    test.to_csv(dataset_test.path + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47d063-176c-4929-914a-dbda828123f3",
   "metadata": {},
   "source": [
    "## Model Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a81724-d889-45c0-be50-80d68288c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",        \n",
    "        \"scikit-learn==1.5.1\",\n",
    "        \"scipy==1.13.1\",\n",
    "        \"numpy==1.26.4\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.4\"\n",
    ")\n",
    "def train_model(\n",
    "    dataset_train: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "):\n",
    "    \"\"\"Training Linear Regressor model for demo_ts.\n",
    "\n",
    "    Args:\n",
    "        dataset_train: The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        model: The model artifact stores the model.joblib file.\n",
    "        metrics: The metrics of the trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time, os, joblib\n",
    "    from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pickle\n",
    "\n",
    "    \n",
    "    with open(dataset_train.path + '.csv', \"r\") as train_data:\n",
    "        dataset_train = pd.read_csv(\n",
    "            train_data,\n",
    "            parse_dates=['Month'],\n",
    "            index_col='Month',\n",
    "        ).to_period('M').reindex()\n",
    "        \n",
    "    dataset_train = dataset_train.sort_values('Month')\n",
    "     \n",
    "    X = dataset_train.drop('y', axis=1)\n",
    "    y = dataset_train['y']\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "    \n",
    "    lrg = LinearRegression(fit_intercept=False)\n",
    "       \n",
    "    regr = Pipeline([\n",
    "        ('regressor', lrg)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    lrg.fit(X_train,\n",
    "            Y_train,\n",
    "           )\n",
    "      \n",
    "    \n",
    "    Y_pred_lrg = lrg.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(Y_test, Y_pred_lrg)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, Y_pred_lrg))\n",
    "    \n",
    "    metrics.log_metric(\"Framework\", \"LinearRegression\")\n",
    "    metrics.log_metric(\"Train_samples_size\", len(X_train))\n",
    "    metrics.log_metric(\"Validation_samples_size\", len(X_test))\n",
    "    metrics.log_metric(\"RMSE\", round(rmse,2))\n",
    "    metrics.log_metric(\"R2 score\", round(r2,2))\n",
    "    \n",
    "    print(\"Linear Regression:\")\n",
    "    print(\"RMSE:\",rmse)\n",
    "    print(\"R2 score:\", r2)\n",
    "    \n",
    "    # Export the model to a file\n",
    "    #os.makedirs(model.path, exist_ok=True)\n",
    "   # joblib.dump(lrg, os.path.join(model.path, \"model.pkl\"))\n",
    "\n",
    "# Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    model_file_path = os.path.join(model.path, \"model.pkl\")\n",
    "\n",
    "    with open(model_file_path, 'wb') as f:\n",
    "        pickle.dump(lrg, f)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7615f28c-7cd9-4e55-9c0f-30d82db53d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",        \n",
    "        \"scikit-learn==1.5.1\",\n",
    "        \"joblib==1.4.2\",\n",
    "        \"numpy==1.26.4\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.4\"\n",
    ")\n",
    "def evaluate_model(\n",
    "    dataset_test: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics]\n",
    ") -> NamedTuple('EvaluationOutput', [('r2', float), ('rmse', float)]):\n",
    "    \"\"\"Evaluate the trained model with test data.\n",
    "\n",
    "    Args:\n",
    "        dataset_test: The testing dataset.\n",
    "        model: The trained model.\n",
    "        \n",
    "    Returns:\n",
    "        metrics: The evaluation metrics of the model.\n",
    "        r2: The R2 score of the model.\n",
    "        rmse: The RMSE of the model.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import pickle\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    try:\n",
    "        # Load the test dataset\n",
    "        with open(dataset_test.path + '.csv', \"r\") as test_data:\n",
    "            test_dataset = pd.read_csv(\n",
    "            test_data,\n",
    "            parse_dates=['Month'],\n",
    "            index_col='Month',\n",
    "        ).to_period('M').reindex()\n",
    "            \n",
    "        test_dataset = test_dataset.sort_values('Month')\n",
    "        \n",
    "        X_test = test_dataset.drop(\"y\", axis=1)\n",
    "        Y_test = test_dataset[\"y\"]\n",
    "        \n",
    "\n",
    "        \n",
    "        # Load the trained model\n",
    "        model_file = model.path + \"/model.pkl\"\n",
    "        with open(model_file, 'rb') as f:\n",
    "            trained_model = pickle.load(f)\n",
    "            \n",
    "        print(f\"Model loaded successfully: {type(trained_model)}\")\n",
    "\n",
    "        # Predict with the model\n",
    "        Y_pred = trained_model.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "        metrics.log_metric(\"RMSE\", round(rmse, 2))\n",
    "        metrics.log_metric(\"R2 score\", round(r2, 2))\n",
    "\n",
    "        print(\"Evaluation results:\")\n",
    "        print(\"RMSE:\", rmse)\n",
    "        print(\"R2 score:\", r2)\n",
    "\n",
    "        from collections import namedtuple\n",
    "        EvaluationOutput = namedtuple('EvaluationOutput', ['r2', 'rmse'])\n",
    "        return EvaluationOutput(r2=float(r2), rmse=float(rmse))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9096ec-cd30-4a26-a63f-636b2f445ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",        \n",
    "        \"scikit-learn==1.5.1\",\n",
    "        \"numpy==1.26.4\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.4\"\n",
    ")\n",
    "def inference(\n",
    "    dataset_test: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    predictions: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Perform inference with the trained model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_test: The testing dataset.\n",
    "        model: The trained model.\n",
    "        \n",
    "    Returns:\n",
    "        predictions: The dataset with predictions.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pickle\n",
    "        \n",
    "    # Load the test dataset\n",
    "    with open(dataset_test.path + '.csv', \"r\") as test_data:\n",
    "        test_dataset = pd.read_csv(\n",
    "            test_data,\n",
    "            parse_dates=['Month'],\n",
    "            index_col='Month',\n",
    "        ).to_period('M').reindex()\n",
    "        \n",
    "    X_test = test_dataset.drop(\"y\", axis=1)\n",
    "\n",
    "    \n",
    "     # Load the trained model\n",
    "    model_file = model.path + \"/model.pkl\"\n",
    "    with open(model_file, 'rb') as f:\n",
    "        trained_model = pickle.load(f)\n",
    "    \n",
    "    # Predict with the model\n",
    "    Y_pred = trained_model.predict(X_test)\n",
    "    \n",
    "    # Save predictions to the output\n",
    "    predictions_df = test_dataset.copy()\n",
    "    predictions_df[\"Predicted_Sales\"] = Y_pred\n",
    "    predictions_df.to_csv(predictions.path + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c95bd09-aba3-430d-b916-427988b9073a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform',\n",
    "        'kfp==2.5.0',\n",
    "        'click>=8.0.0,<9',\n",
    "        'kfp-pipeline-spec==0.2.2',\n",
    "        'kfp-server-api>=2.0.0,<2.1.0',\n",
    "        'kubernetes>=8.0.0,<27',\n",
    "        'PyYAML>=5.3,<7',\n",
    "        'requests-toolbelt>=0.8.0,<1',\n",
    "        'tabulate>=0.8.6,<1',\n",
    "        'protobuf>=3.13.0,<4',\n",
    "        'urllib3<2.0.0',\n",
    "        \"numpy==1.26.4\",\n",
    "        \"joblib==1.3.2\",\n",
    "        \"scikit-learn==1.5.1\",\n",
    "    ],\n",
    "    base_image='python:3.10.4'\n",
    ")\n",
    "def deploy_model_to_endpoint(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_display_name: str,\n",
    "    model: Input[Model],\n",
    "    endpoint_display_name: str,\n",
    "    machine_type: str = 'n1-standard-2',\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import os\n",
    "    import pickle\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # Get the directory containing the model file\n",
    "    model_dir = model.path\n",
    "\n",
    "    model_file = os.path.join(model_dir, \"model.pkl\")\n",
    "\n",
    "    print(f\"Model directory: {model_dir}\")\n",
    "    print(f\"Model file: {model_file}\")\n",
    "    print(f\"Model path: {Path(model_dir).parent}\")\n",
    "\n",
    "    # Load the model to verify it\n",
    "    try:\n",
    "        with open(model_file, 'rb') as f:\n",
    "            loaded_model = pickle.load(f)\n",
    "        print(f\"Model loaded successfully: {type(loaded_model)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "    # Upload the model to Vertex AI\n",
    "    uploaded_model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=model_dir,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "    )\n",
    "\n",
    "    # Create the endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)\n",
    "\n",
    "    # Deploy the model to the endpoint\n",
    "    deployment = endpoint.deploy(\n",
    "        model=uploaded_model,\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count\n",
    "    )\n",
    "\n",
    "    print(f\"Model deployed to endpoint {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf9e86b-78f8-4118-a4a5-7f6ab6f3b51f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2627/2304684031.py:28: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(evaluate_model_task.outputs['r2'] > 0.8):\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"demo-time-series\",\n",
    ")\n",
    "def pipeline(\n",
    "    PROJECT_ID: str,\n",
    "    DATASET_ID: str,\n",
    "    TABLE_TRAIN: str,\n",
    "    TABLE_TEST: str,\n",
    "):\n",
    "    export_dataset_task = export_datasets(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        table_train=TABLE_TRAIN,\n",
    "        table_test=TABLE_TEST,\n",
    "    )\n",
    "    \n",
    "    train_model_task = train_model(\n",
    "        dataset_train=export_dataset_task.outputs[\"dataset_train\"],\n",
    "        \n",
    "    )\n",
    "    \n",
    "    evaluate_model_task = evaluate_model(\n",
    "        dataset_test=export_dataset_task.outputs[\"dataset_train\"],\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with dsl.Condition(evaluate_model_task.outputs['r2'] > 0.8):\n",
    "        inference_task = inference(\n",
    "            dataset_test=export_dataset_task.outputs[\"dataset_test\"],\n",
    "            model=train_model_task.outputs[\"model\"],   \n",
    "            \n",
    "        )\n",
    "        \n",
    "        deploy_model_task = deploy_model_to_endpoint(\n",
    "            project=PROJECT_ID,\n",
    "            location=REGION,\n",
    "            model=train_model_task.outputs['model'],\n",
    "            model_display_name=\"demo-time-series\",\n",
    "            endpoint_display_name=\"demo-time-series-endpoint\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3309a900-1c8a-4240-a96b-66b583a60f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"../src/pipe_comps/demo_ts_pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b39d75-8ba0-4c75-909b-c27c91d6ac5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/502688298240/locations/us-central1/pipelineJobs/demo-time-series-20240912134420\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/502688298240/locations/us-central1/pipelineJobs/demo-time-series-20240912134420')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/demo-time-series-20240912134420?project=502688298240\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"demo_ts\",\n",
    "    template_path=\"../src/pipe_comps/demo_ts_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'PROJECT_ID': PROJECT_ID,\n",
    "        'DATASET_ID': DATASET_ID,\n",
    "        'TABLE_TRAIN': TABLE_TRAIN,\n",
    "        'TABLE_TEST': TABLE_TEST\n",
    "    },\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fdd1522-4d92-488b-a883-d591c501319a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_registry = RegistryClient(host=f\"https://us-central1-kfp.pkg.dev/demoespecialidadgcp/time-series-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a067bc-1cbd-49b4-8c2e-011b4f90f8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "templateName, versionName = client_registry.upload_pipeline(\n",
    "  file_name=\"../src/pipe_comps/demo_ts_pipeline.yaml\",\n",
    "  tags=[\"latest\"],\n",
    "  extra_headers={\"description\":\"Pipeline para la transformacion de datos, entrenamiento de modelos, registro de metricas, predicciones y modelo, y despliegue del modelo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc27c1-0e8c-4f77-b72d-4bc604420f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
